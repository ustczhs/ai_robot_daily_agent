#!/usr/bin/env python3
"""
æµ‹è¯•Ollamaé…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import yaml
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from langchain_ollama import OllamaLLM
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·å®‰è£… langchain-ollama")
    print("pip install langchain-ollama")
    sys.exit(1)


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path('./config/config.yaml')
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def test_ollama_connection():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•Ollamaè¿æ¥")
    print("=" * 60)

    try:
        config = load_config()

        # æ£€æŸ¥é…ç½®
        provider = config['llm']['provider'].lower()
        if provider != 'ollama':
            print(f"âš ï¸  å½“å‰é…ç½®ä½¿ç”¨çš„æ˜¯ {provider}ï¼Œä¸æ˜¯ollama")
            print("   è¦æµ‹è¯•Ollamaï¼Œè¯·åœ¨config.yamlä¸­è®¾ç½®:")
            print("   llm:")
            print("     provider: ollama")
            print("     model: qwen3:4b-instruct  # æˆ–å…¶ä»–æœ¬åœ°æ¨¡å‹")
            return False

        model_name = config['llm']['model']
        base_url = config['llm'].get('ollama_base_url', 'http://localhost:11434')

        print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   æœåŠ¡åœ°å€: {base_url}")
        print(f"   æ¸©åº¦: {config['llm']['temperature']}")

        # åˆ›å»ºOllamaå®ä¾‹
        print("\nğŸ”— è¿æ¥OllamaæœåŠ¡...")
        llm = OllamaLLM(
            model=model_name,
            base_url=base_url,
            temperature=config['llm']['temperature'],
            reasoning=False,
            num_gpu=999,              # å¼ºåˆ¶å°½é‡å¤šå±‚å¸è½½åˆ° GPUï¼ˆ999 è¡¨ç¤ºå…¨éƒ¨å¯èƒ½ï¼‰
            num_thread=12,            # æ ¹æ®ä½ çš„ CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼ˆ4060 Ti æ­é…çš„ CPU ä¸€èˆ¬ 8-16 æ ¸ï¼‰
            num_predict=512,          # é™åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œé¿å…ä¸å¿…è¦é¢„åˆ†é…
            num_ctx=8192,   
        )

        # æµ‹è¯•åŸºæœ¬è°ƒç”¨
        print("ğŸ’¬ å‘é€æµ‹è¯•æ¶ˆæ¯...")
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œé™åˆ¶åœ¨50å­—ä»¥å†…ã€‚"
        enhanced_prompt = test_prompt + " /no_think"
        # è¾“å‡ºæ¨ç†æ—¶é—´
        import time
        start_time = time.time()
        response = llm.invoke(enhanced_prompt)
        end_time = time.time()
        inference_time = end_time - start_time
        print(f"æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")

        print("âœ… Ollamaè¿æ¥æˆåŠŸ!")
        print(f"ğŸ¤– æ¨¡å‹å›å¤: {response}")

        # éªŒè¯å›å¤è´¨é‡
        if len(response.strip()) > 10:
            print("âœ… å›å¤å†…å®¹æ­£å¸¸")
        else:
            print("âš ï¸  å›å¤å†…å®¹è¾ƒçŸ­")

        return True

    except Exception as e:
        print(f"âŒ Ollamaæµ‹è¯•å¤±è´¥: {str(e)}")

        print("\nğŸ”§ æ•…éšœæ’é™¤:")
        print("1. ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ:")
        print("   ollama serve")
        print("")
        print("2. ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½:")
        print(f"   ollama pull {config['llm']['model']}")
        print("")
        print("3. æ£€æŸ¥æœåŠ¡åœ°å€:")
        print(f"   curl {config['llm']['ollama_base_url']}/api/tags")
        print("")
        print("4. é…ç½®æ–‡ä»¶ç¤ºä¾‹:")
        print("   llm:")
        print("     provider: ollama")
        print("     model: qwen3:4b-instruct")
        print("     ollama_base_url: http://localhost:11434")
        print("     temperature: 0.7")

        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Ollamaé…ç½®æµ‹è¯•")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")

    success = test_ollama_connection()

    if success:
        print("\nğŸ‰ Ollamaé…ç½®æµ‹è¯•é€šè¿‡!")
        print("ç°åœ¨å¯ä»¥å°†config.yamlä¸­çš„provideræ”¹ä¸ºollamaæ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥!")
        print("è¯·æ£€æŸ¥Ollamaå®‰è£…å’Œé…ç½®")


if __name__ == "__main__":
    main()
